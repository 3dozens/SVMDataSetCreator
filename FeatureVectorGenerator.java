import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

import net.reduls.igo.Morpheme;
import net.reduls.igo.Tagger;

public class FeatureVectorGenerator {

	public Map<String, double[]> generateTFIDFVectors(List<String> documents) {

		// List<word>
		List<String> wordList = new ArrayList<String>();

		// Map<document, Map<word, tf>>
		Map<String, Map<String, Integer>> tfMap = new HashMap<>();

		// Map<word, df>
		Map<String, Integer> dfMap = new HashMap<>();

		for (String document : documents) {

			List<String> parsedWords = parse(document);

			Map<String, Integer> docTFMap = new HashMap<>();
			for (String word : parsedWords) {

				// ï¿½Pï¿½ê–ˆï¿½ï¿½TFï¿½ï¿½ï¿½vï¿½Z
				if (docTFMap.containsKey(word)) {
					int tf = docTFMap.get(word);
					docTFMap.put(word, tf + 1);
				} else {
					docTFMap.put(word, 1);
				}

				// ï¿½Pï¿½ï¿½ê——ï¿½É’Pï¿½ï¿½ï¿½Ç‰ï¿½
				if (!wordList.contains(word)) {
					wordList.add(word);
				}
			}

			tfMap.put(document, docTFMap);

			// DFï¿½ï¿½ï¿½vï¿½Z
			for (String word : docTFMap.keySet()) {
				if (dfMap.containsKey(word)) {
					int df = dfMap.get(word);
					dfMap.put(word, df + 1);
				} else {
					dfMap.put(word, 1);
				}
			}
		}

		Map<String, double[]> featureVectors = new LinkedHashMap<String, double[]>();
		for (String document : documents) {

			// Map<word, tfidf>
			Map<String, Double> docTFIDFMap = new HashMap<String, Double>();

			// ï¿½Pï¿½ê–ˆï¿½ï¿½TF-IDFï¿½ï¿½ï¿½ï¿½ï¿½ß‚ï¿½
			Map<String, Integer> docTFMap = tfMap.get(document);
			for (String word : docTFMap.keySet()) {

				double tf = (double) docTFMap.get(word);
				double df = (double) dfMap.get(word);
				double N = (double) documents.size();

				double tfidf = tf * Math.log(N / df);

				docTFIDFMap.put(word, tfidf);
			}

			// ï¿½ï¿½ï¿½ï¿½ï¿½xï¿½Nï¿½gï¿½ï¿½ï¿½ğ¶ï¿½
			double[] featureVector = new double[wordList.size()];
			for (int i = 0; i < wordList.size(); i++) {
				String word = wordList.get(i);
				if (docTFIDFMap.containsKey(word)) {
					featureVector[i] = docTFIDFMap.get(word);
				} else {
					featureVector[i] = 0;
				}
			}
			featureVectors.put(document, featureVector);
		}

		// ï¿½Pï¿½ï¿½ê——ï¿½ï¿½\ï¿½ï¿½ comment outed by Hayasaka
//		System.out.println("=== word list ===");
//		System.out.print("(");
//		for (int i = 0; i < words.size(); i++) {
//			System.out.print((i + 1) + ":" + words.get(i));
//			if (i != words.size() - 1) {
//				System.out.print(", ");
//			}
//		}
//		System.out.println(")");
//		System.out.println(""); 

		return featureVectors;
	}

	/**
	 * æ¸¡ã•ã‚ŒãŸæ–‡å­—åˆ—ã‚’å½¢æ…‹ç´ è§£æã—ã€å‰¯è©ã€åŠ©è©ã€åŠ©å‹•è©ã€æ•°ã§ãªã„
	 * å˜èªã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™
	 * @param str å¯¾è±¡ã®æ–‡å­—åˆ—
	 * @return å˜èªãƒªã‚¹ãƒˆ
	 */
	private List<String> parse(String str) {

		Tagger tagger = null;
		try {
			tagger = new Tagger("ipadic");
		} catch (Exception e) {
			e.printStackTrace();
		}

		List<Morpheme> morphemes = tagger.parse(str);
		List<String> words = morphemes.stream()
			.filter(m -> !m.feature.contains("å‰¯è©") && !m.feature.contains("åŠ©è©") 
				&& !m.feature.contains("åŠ©å‹•è©") && !m.feature.contains("æ•°"))
			.map(m -> m.surface)
			.collect(Collectors.toList());

		return words;
	}
	
	public double calcCosineSimilarity(double[] vector1, double[] vector2) {
		
		return calcInnerProduct(vector1, vector2) / 
				(calcMagnitude(vector1) * calcMagnitude(vector2));
	}
	
	private double calcInnerProduct(double[] vector1, double[] vector2) {
		
		double result = 0.0;
		for(int i = 0; i < vector1.length; i++) {
			result += vector1[i] * vector2[i];
		}
		
		return result;
	}
	
	/**
	 * ãƒ™ã‚¯ãƒˆãƒ«ã®å¤§ãã•ã‚’æ±‚ã‚ã¾ã™
	 */
	private double calcMagnitude(double[] vector) {
		
		double magnitudeBeforeSqrt = Arrays.stream(vector)
			.map(v -> Math.pow(v, 2))
			.sum();
		
		double magnitude = Math.sqrt(magnitudeBeforeSqrt);
		
		return magnitude;
	}
}
